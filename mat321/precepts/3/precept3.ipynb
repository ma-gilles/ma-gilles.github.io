{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precept 3: Matrix norms and SVD\n",
    "## Task 1: Matrix norms\n",
    "\n",
    "The matrix A $\\in \\mathbb{R}^{n\\times n} $ defined by:\n",
    "$$\n",
    "A = \\frac{1}{n+1}\\begin{bmatrix}\n",
    "0 & -1 & &   & &  \\\\\n",
    "1 & 0 & -1  &  & &  \\\\\n",
    "  & 1 & 0 & -1  & &  \\\\\n",
    "& & \\ddots & \\ddots & \\ddots & \\\\\n",
    " & & &1  & 0 & -1 \\\\\n",
    "  & & & & 1 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "is called the first-order central difference matrix.\n",
    "\n",
    "Derive (by hand) its operator 1-norm, operator $\\infty$-norm and Frobenius norm. Compute its operator 2-norm (with code) for n = 100. You may the function `np.linalg.svd`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# You may use this function np.linalg.svd\n",
    "# help(np.linalg.svd)\n",
    "# And this function to build the matrix\n",
    "B = np.diag(np.array([1,2,3]), -1)\n",
    "print(\"B=\")\n",
    "print(B)\n",
    "# help(np.diag)\n",
    "\n",
    "\n",
    "# Form the matrix\n",
    "n = 100\n",
    "A = 0 # Complete this\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: SVD and eigenvalues\n",
    "\n",
    "In class we discussed the link between the SVD of A and the eigenvalue decomposition of $A^TA$. One way to compute the SVD of A is in fact to use an algorithm for the eigenvalue decomposition of $A^TA$. It turns out there is a slightly better choice, which we explore here.\n",
    "\n",
    "### 1. Ill conditioning\n",
    "Find the condition number of $A^TA$ in terms of the condition number of $A \\in \\mathcal{R}^{n \\times n} $. Recall that the condition number is defined as:\n",
    "$$\n",
    "\\kappa_2(A) = \\frac{\\sigma_1}{\\sigma_n}\n",
    "$$\n",
    "\n",
    "\n",
    "Why might this be bad for computation?\n",
    "### 2. A better choice\n",
    "Suppose $A \\in \\mathbb{R}^{m \\times m}$ has an SVD of $A = U \\Sigma V^T$. Let B be defined as:\n",
    "$$\n",
    "B = \\begin{bmatrix}\n",
    "0 &  A \\\\\n",
    "A^T & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Verify that $B = \\hat{V}\\hat{\\Sigma}\\hat{V}^{T} $ is an eigenvalue decomposition, where:\n",
    "$$\n",
    "\\hat{V} = \\frac{1}{\\sqrt{ 2 } }\\begin{bmatrix}\n",
    "U & U \\\\\n",
    "V & -V \\\\\n",
    " \\end{bmatrix} \\qquad \\hat{\\Sigma} = \\begin{bmatrix}\n",
    "\\Sigma & 0 \\\\\n",
    "0 & -\\Sigma \\\\\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "What is the condition number of $B$ in terms of the condition number of A? (Use the fact that if A is symmetric, the singular values of A are the absolute value of the eigenvalues of A)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Testing out task 2\n",
    "\n",
    "Here, we implement the above algorithm for the SVD, using the algorithm for eigenvalue decomposition as a black box. We will cover algorithm for eigenvalue decomposition later in the class.\n",
    "\n",
    "First, we generate some matrices with known condition number: $\\rho^{100}$. For $ \\rho \\approx 1$ this is well conditioned but with $\\rho$ a little bit bigger than one (say 1.2), this is highly ill conditioned. We start with $\\rho = 1.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n = 100\n",
    "\n",
    "# Generating an ill-conditioned matrix\n",
    "# Forms two random orthogonal matrix (don't worry about how this works)\n",
    "U,_ = np.linalg.qr( np.random.randn(n,n) )\n",
    "V,_ = np.linalg.qr( np.random.randn(n,n) )\n",
    "rho = 1.01\n",
    "S = rho**(np.arange(n, 0, -1)) # Geometrically growing singular values\n",
    "# Form a new matrix, with random singular vectors U and Vh and the new eigenvalues\n",
    "A = (U * S ) @ V.T\n",
    "print('condition number is:', S[0] / S[-1])\n",
    "plt.semilogy(S, 'x')\n",
    "plt.ylabel('singular values')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the algorithm suggested in task 2.1 in my_unstable_SVD.\n",
    "\n",
    "Make sure it is an SVD of A: U and V are orthogonal and $A = USV^T$\n",
    "In principle, we should also impose that the entries of S are decreasing but we won't worry about this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can sue this function:\n",
    "# np.linalg.eigh computes the eigenvalue decomposition for hermitian (and symmetric) matrices\n",
    "#help(np.linalg.eigh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the SVD of A from the eigenvalue decomposition of A.T @ A\n",
    "def my_unstable_SVD(A):\n",
    "    # Compute the right singular vectors and singular values\n",
    "    # of A using an an eigendecomposition of A^T A\n",
    "    # Then, use that U = A V D^{-1} to compute the left singular vectors\n",
    "    U,S,V = np.zeros_like(A),np.zeros_like(A[:,0]),np.zeros_like(A)\n",
    "\n",
    "    # Returns orthogonal matrices U and V, and a vector S such that A  = U @ np.diag(S) @ V.T = U*S @ V.T\n",
    "    return U, S, V\n",
    "\n",
    "\n",
    "def check_orthogonal(Q):\n",
    "    n = Q.shape[0]\n",
    "    err = np.linalg.norm(Q.T @ Q - np.eye(n)) \n",
    "    return err\n",
    "\n",
    "U, S, V = my_unstable_SVD(A)\n",
    "print(\"|UU.T - I| = \", check_orthogonal(U))\n",
    "print(\"|VV.T - I| = \", check_orthogonal(V))\n",
    "print(\"|A - USV.T| = ?\", np.linalg.norm(A - (U * S) @ V.T ) / np.linalg.norm(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now repeat with a more ill conditioned matrix by setting $\\rho = 1.2$ and $\\rho = 1.3$ What do you notice?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cauchy",
   "language": "python",
   "name": "cauchy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
