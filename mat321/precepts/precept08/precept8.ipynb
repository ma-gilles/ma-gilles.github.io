{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precept 8: Eigenvalue iterations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "In this precept, we will investigate power method and\n",
    "inverse power method for symmetric matrices.\n",
    "\n",
    "We start by creating an example matrix. We $1000$\n",
    "uniformly random points on the unit sphere by generating Gaussian points using `np.random.randn` and\n",
    "dividing by their length. Then we make a scatter plot of the points.\n",
    "\n",
    "Your task is to compute the $n \\times\n",
    "n$ matrix $D$ whose $(i,j)$-th entry is the distance between $X[i,:]$ and\n",
    "$X[j,:]$, and define\n",
    "$$\n",
    "A(i,j) = \\exp(-D(i,j)^2).\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# n random vectors on the sphere\n",
    "n = 1000\n",
    "X = np.random.randn(n,3)\n",
    "X = X / np.linalg.norm(X, axis =1,keepdims=True)\n",
    "\n",
    "\n",
    "# Scatter plot\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X[:,0], X[:,1], X[:,2])\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "plt.show()\n",
    "\n",
    "## Compute the matrix A :\n",
    "A = np.zeros((n,n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 2 \n",
    "Using the built in numpy command compute the\n",
    "built in eigenvalues and eigenvectors of $A$. Sort the eigenvalues is descending\n",
    "order and sort the columns of $V$ in a corresponding way. Check \n",
    "$$\n",
    "\\text{norm}(A - V D V^\\top )/\\text{norm}(A)\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\text{norm}(I - V^\\top V)\n",
    "$$\n",
    "where $I$ is the identity matrix, to make sure that you have decomposed $A$ into\n",
    "$V D V^\\top$, where $V$ is orthogonal and $D$ is diagonal, which is the\n",
    "decomposition guaranteed by the spectral theorem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 use the following function to compute eigenvalues and eigenvectors of A\n",
    "help(np.linalg.eigh)\n",
    "help(np.flip) # May want to use this function to reorder eigenvalues/vectors\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Task 3\n",
    "Visualize the eigenvectors in $V$. In particular, the\n",
    "eigenvectors $V[:,j]$ are vectors in $\\mathbb{R}^n$. Make a scatter plot of $X$\n",
    "and color the scatter plot using $V_2$, the second eigenvector (equal to $V[:,1]$ in numpy), the eigenvector associated with the\n",
    "second largest eigenvalue\n",
    "These eigenvectors are very rough approximations of spherical harmonics \n",
    "https://en.wikipedia.org/wiki/Spherical_harmonics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: scatter plot of eigenvectors\n",
    "# Scatter plot\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 4\n",
    "First, use the power method to find $V_1$ (the first eigenvector, associated with the greatest eigenvalue). That is, start\n",
    "with a random vector $v$ and keep applying the matrix and normalizing until you\n",
    "start to converge to $v$. You can check the error by\n",
    "$$\n",
    "\\min(\\text{norm}(v - V_1),\\text{norm}(v + V_1))\n",
    "$$\n",
    "the reason that we need to check $-v$ and $v$ is that eigenvectors are only\n",
    "defined up to multiplication by a constant. Even when $v$ is normalized to have\n",
    "length $1$ there is still a sign ambiguity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 implement power method\n",
    "\n",
    "def distance_to_eigenvector(vk, eigenvec):\n",
    "    return np.min([np.linalg.norm(vk - eigenvec), np.linalg.norm(vk + eigenvec)])\n",
    "\n",
    "\n",
    "err = np.zeros(100)\n",
    "\n",
    "## add your code\n",
    "plt.semilogy(err)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 5\n",
    " Next, use the power method to find $V_2$ (the second eigenvector) using the\n",
    "following trick: at each iteration, before multiplying by $A$, project the\n",
    "vector $v$ to the space orthogonal to $V_1$. Theoretically, it would suffice\n",
    "to just project $v$ orthogonal to the space $V_1$ initially, but to avoid any\n",
    "numerical issues perform the projection each iteration. Recall that the project\n",
    "orthogonal to a vector $u$ which is normalized to have unit length is \n",
    "$$\n",
    "w = v - u(u^\\top v)\n",
    "$$\n",
    "Check that you are able to get $V(:,2)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5 implement power method for v2\n",
    "\n",
    "def distance_to_eigenvector(vk, eigenvec):\n",
    "    return np.min([np.linalg.norm(vk - eigenvec), np.linalg.norm(vk + eigenvec)])\n",
    "\n",
    "\n",
    "err = np.zeros(100)\n",
    "\n",
    "## add your code\n",
    "plt.semilogy(err)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 6\n",
    "\n",
    "Next, using the eigenvalues $d$ that you computed using the\n",
    "built in solver, use the inverse power method to compute $V_1$ and $V_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement task 6\n",
    "\n",
    "# add your code\n",
    "\n",
    "print(distance_to_eigenvector(v1, V[:,0]))\n",
    "\n",
    "# add your code\n",
    "\n",
    "print(distance_to_eigenvector(v2, V[:,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
